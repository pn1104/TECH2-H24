{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Business cycle correlations\n",
    "\n",
    "For this exercise, you'll be using macroeconomic data from the folder `../data/FRED`.\n",
    "\n",
    "1.  There are seven decade-specific files named `FRED_monthly_19X0.csv` where `X` identifies the decade (`X` takes on the values 5, 6, 7, 8, 9, 0, 1). Write a loop that reads in all seven files as DataFrames and store them in a list.\n",
    "\n",
    "    *Hint:* Recall from the lecture that you should use <TT>pd.read_csv(..., parse_dates=['DATE'])</TT> to automatically parse strings stored in the <TT>DATE</TT> column as dates.\n",
    "2.  Use [`pd.concat()`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to concate these data sets into a single `DataFrame` and set the `DATE` column as the index.\n",
    "3.  You realize that your data does not include GDP since this variable is only reported at quarterly frequency.\n",
    "    Load the GDP data from the file `GDP.csv` and merge it with your monthly data using an _inner join_.\n",
    "4.  You want to compute how (percent) changes of the variables in your data correlate with percent changes in GDP.\n",
    "\n",
    "    1. Create a _new_ `DataFrame` which contains the percent changes in CPI and GDP (using \n",
    "    [`pct_change()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pct_change.html), \n",
    "    see also the last exercise in workshop 3),\n",
    "    and the absolute changes for the remaining variables (using \n",
    "    [`diff()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.diff.html)).\n",
    "    2.  Compute the correlation of the percent changes in GDP with the (percent) changes of all other variables (using [`corr()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)). What does the sign and magnitude of the correlation coefficient tell you?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Exercise: Loading many data files\n",
    "\n",
    "In the previous exercise, you loaded the individual files by specifing an explicit list of file names. This can become tedious or infeasible if your data is spread across many files with varying file name patterns. Python offers the possibility to iterate over all files in a directory (for example, using [`os.listdir()`](https://docs.python.org/3/library/os.html#os.listdir)),\n",
    "or to iterate over files that match a pattern, for example using [`glob.glob()`](https://docs.python.org/3/library/glob.html).\n",
    "\n",
    "Repeat parts (1) and (2) from the previous exercise, but now iterate over the input files using \n",
    "[`glob.glob()`](https://docs.python.org/3/library/glob.html). You'll need to use a wildcard `*` and make sure to match only the relevant files in `../data/FRED`, i.e., those that start with `FRED_monthly`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Exercise: Decade averages of macro time series\n",
    "\n",
    "\n",
    "For this exercise, you'll be using macroeconomic data from the folder `../data/FRED`.\n",
    "\n",
    "1.  There are five files containing monthly observations on annual inflation (INFLATION), the Fed Funds rate (FEDFUNDS), the labor force participation rate (LFPART), the 1-year real interest rate (REALRATE) and the unemployment rate (UNRATE).\n",
    "    Write a loop to import these and merge them on `DATE` into a single `DataFrame` using _outer joins_ (recall that [`merge()`](https://pandas.pydata.org/docs/reference/api/pandas.merge.html) \n",
    "    and [`join()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html) operate on only two DataFrames at a time). \n",
    "\n",
    "    *Hint:* Recall from the lecture that you should use <TT>pd.read_csv(..., parse_dates=['DATE'])</TT> to automatically parse strings stored in the <TT>DATE</TT> column as dates.\n",
    "\n",
    "2.  Your friend is a pandas guru and tells you that you don't need to iteratively merge many files but can instead directly use [`pd.concat()`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) for merging many DataFrames in a single step.\n",
    "    Repeat the previous part using `pd.concat()` instead, and verify that you get the same result (you can do this using [`compare()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.compare.html)).\n",
    "3.  You want to compute the average value of each variable by decade, but you want to include only decades without _any_ missing values for _all_ variables.\n",
    "    1.  Create a variable `Decade` which stores the decade (1940, 1950, ...) for each observation.\n",
    "\n",
    "        *Hint:* You should have set the `DATE` as the `DataFrame` index. Then you can access the calendar year using the attribute `df.index.year` which can be used to compute the decade.\n",
    "    2.  Write a function `num_missing(x)` which takes as argument `x` a `Series` and returns the number of missing values in this `Series`.\n",
    "    3.  Compute the number of missing values by decade for each variable using a [`groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) operation and the function `num_missing` you wrote.\n",
    "    4.  Aggregate this data across all variables to create an indicator for each decade whether there are any missing values. This can be done in many ways but will require aggregation across columns, e.g., with `sum(..., axis=1)`.\n",
    "    5.  Merge this decade-level indicator data back into the original `DataFrame` (_many-to-one_ merge). \n",
    "4.  Using this indicator, drop all observations which are in a decade with missing values.\n",
    "5.  Compute the decade average for each variable.\n",
    "\n",
    "**Challenge**\n",
    "\n",
    "-   Your pandas guru friend claims that all the steps in 3.2 to 3.5 can be done with a single one-liner using [`transform()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.transform.html). Can you come up with a solution?\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Exercise: Mering the Titanic data\n",
    "\n",
    "In this exercise, you'll be working with the the original Titanic data set in `titanic.csv` and additional (partly fictitious) information on passengers stored in `titanic-additional.csv`, both located in the `data/` folder.\n",
    "\n",
    "The goal of the exercise is to calculate the survival rates by country of residence (for this exercise we restrict ourselves to the UK, so these will be England, Scotland, etc.).\n",
    "\n",
    "1.  Load the `titanic.csv` and `titanic-additional.csv` into two DataFrames.\n",
    "\n",
    "    Inspect the columns contained in both data sets. As you can see, the original data contains the full name including the title\n",
    "    and potentially maiden name (for married women) in a single column.\n",
    "    The additional data contains this information in separate columns.\n",
    "    You want to merge these data sets, but you first need to create common keys in both DataFrames.\n",
    "\n",
    "2.  Since the only common information is the name, you'll need to extract the individual name components from the original DataFrame\n",
    "    and use these as merge keys.\n",
    "\n",
    "    Focusing only on men (who have names that are much easier to parse), split the `Name` column into the tokens \n",
    "    `Title`, `FirstName` and `LastName`, just like the columns in the second DataFrame.\n",
    "\n",
    "    *Hint:* This is the same task as in the last exercise in Workshop 2. You can just use your solution here.\n",
    "\n",
    "3.  Merge the two data sets based on the columns `Title`, `FirstName` and `LastName` you just created using a _left join_ (_one-to-one_ merge).\n",
    "    Tabulate the columns and the number of non-missing observations to make sure that merging worked. \n",
    "\n",
    "    *Note:* The additional data set contains address information only for passengers from the UK, so some of these fields will be missing.\n",
    "\n",
    "4.  You are now in a position to merge the country of residence (_many-to-one_ merge). Load the country data from `UK_post_codes.csv` which contains \n",
    "    the UK post code prefix (which you can ignore), the corresponding city, and the corresponding country.\n",
    "\n",
    "    Merge this data with your passenger data set using a _left join_ (what is the correct merge key?).\n",
    "\n",
    "5.  Tabulate the number of observations by `Country`, including the number of observations with missing `Country` (these are passengers residing outside the UK).\n",
    "\n",
    "    Finally, compute the mean survival rate by country."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
